{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Firsts\r\n",
    "\r\n",
    "If we consider all the messages ever sent to, and recieved by, _the corpus_, when did each word enter the corpus? Who put it there? What does it say about a person if they put a lot of new words into the corpus, and what even is a word? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "Load up a tonne of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\r\n",
    "import json\r\n",
    "import os\r\n",
    "import pickle\r\n",
    "import random\r\n",
    "import re\r\n",
    "import textwrap\r\n",
    "from pathlib import Path\r\n",
    "from collections import OrderedDict\r\n",
    "\r\n",
    "import matplotlib as mpl\r\n",
    "import matplotlib.dates as mdates\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from matplotlib.font_manager import FontProperties\r\n",
    "from matplotlib.ticker import MultipleLocator, FixedFormatter, FixedLocator\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import seaborn as sns\r\n",
    "from scipy.optimize import curve_fit\r\n",
    "from scipy.spatial import ConvexHull\r\n",
    "\r\n",
    "import message_helpers as mh\r\n",
    "from hangouts_loader import load_hangouts\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\r\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Segoe UI Emoji\"]\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_name = \"all_convo.pickle\"\r\n",
    "pickle_path = Path(pickle_name)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your name here. This is so that you can take yourself out of some of the graphs. Because these are conversations, naievely, they go A B A B and so on, so you'll be roughly 50% of the messages, which makes other trends hard to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_NAME = \"Ben Doherty\"\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_convo_df = pd.read_pickle(pickle_path)\r\n",
    "print(f\"done: all_convo_df has {all_convo_df.shape[0]} rows\")\r\n",
    "all_convo_df.head()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\r\n",
    "    f\"Overall, there are {len(all_convo_df)}, messages in this dataset. \"\r\n",
    "    f\"These come from about {len(all_convo_df.sender_name.unique())} people, \"\r\n",
    "    f\"covering a period of {str(all_convo_df.datetime.max()-all_convo_df.datetime.min()).split(' days')[0]} days \"\r\n",
    "    f\"between {all_convo_df.datetime.min():%B, %Y} and {all_convo_df.datetime.max():%B, %Y}. \"\r\n",
    "    f\"Over {len(all_convo_df.platform.unique())} platforms:\"\r\n",
    ")\r\n",
    "all_convo_df.platform.value_counts()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"firsts.pickle\"):\r\n",
    "    firsts_df = pd.read_pickle(\"firsts.pickle\")\r\n",
    "else:\r\n",
    "    firsts = []\r\n",
    "    firsts_dicts = []\r\n",
    "    url_regex = r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\"\r\n",
    "\r\n",
    "    for i, row in all_convo_df.iterrows():\r\n",
    "        merged = str(row.content).replace(\"'\", \"\")\r\n",
    "        no_urls = re.sub(url_regex, \" \", merged)\r\n",
    "        just_words = re.sub(r\"[^\\w]\", \" \", no_urls).split()\r\n",
    "        unique_words = set(just_words)\r\n",
    "        for word in unique_words:\r\n",
    "            w = str(word).lower()\r\n",
    "            if (not w.isnumeric()) and (w not in firsts):\r\n",
    "                # print(f\"|{word}|\", \"first in:\", row.content, row.datetime)\r\n",
    "                d = dict(\r\n",
    "                    word=w,\r\n",
    "                    message=str(row.content),\r\n",
    "                    datetime=row.datetime,\r\n",
    "                    by=row.sender_name,\r\n",
    "                    intials=row.initials,\r\n",
    "                )\r\n",
    "                firsts.append(w)\r\n",
    "                firsts_dicts.append(d)\r\n",
    "\r\n",
    "\r\n",
    "    firsts_df = pd.DataFrame(firsts_dicts).set_index(\"datetime\")\r\n",
    "    firsts_df.to_pickle(\"firsts.pickle\")\r\n",
    "\r\n",
    "firsts_df\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsts_df.by.value_counts().tail(5)#[:30]\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsts_df[firsts_df.word == \"my\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsts_df[firsts_df.by == \"Ayelen Moure\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most unique message\r\n",
    "\r\n",
    "Which message has the most first time uses in it? In my case it's \r\n",
    "\r\n",
    "> At the cost of punching myself in the chest so hard I nearly broke a rib/stopped my heart, I managed to do a couple of good superman-carves into flat 180. They look rad, if I don't crash (about ⅛ of the time)\r\n",
    "\r\n",
    "which if I capitalise the firsts, is:\r\n",
    "\r\n",
    "> At the COST of PUNCHING MYSELF in the CHEST so HARD I NEARLY BROKE a RIB STOPPED MY HEART  I MANAGED to do a COUPLE of good SUPERMAN CARVES INTO FLAT 180  They LOOK RAD  IF I don t CRASH  ABOUT ⅛ of the TIME \r\n",
    "\r\n",
    "Pretty amazing that nobody said \"my\" until 2018 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc = firsts_df.message.value_counts()\r\n",
    "# fc[[(len(x)<500) for x in fc.index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_unique_message = \"At the cost of punching myself in the chest so hard I nearly broke a rib/stopped my heart, I managed to do a couple of good superman-carves into flat 180. They look rad, if I don't crash (about ⅛ of the time)\"\r\n",
    "# # um = most_unique_message.split(\" /-\")\r\n",
    "# um = re.split(\"[ -/]\", most_unique_message, flags=re.IGNORECASE)\r\n",
    "# for i, word in enumerate(um):\r\n",
    "#     try:\r\n",
    "#         m = firsts_df[firsts_df.word == word].message[0]\r\n",
    "#         # print(word, m)\r\n",
    "#         if m == most_unique_message:\r\n",
    "#             um[i] = word.upper()\r\n",
    "#     except:\r\n",
    "#         pass\r\n",
    "# \" \".join(um)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts= pd.Timestamp(\"2018-01-27 12:42:56.523\")\r\n",
    "# start = ts-pd.Timedelta(minutes=1)\r\n",
    "# end = ts+pd.Timedelta(minutes=25)\r\n",
    "# all_convo_df[(all_convo_df.datetime >start) & (all_convo_df.datetime < end)][[\"sender_name\", \"content\",\"source_convo\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = firsts_df.groupby(pd.Grouper(freq=\"w\"))\r\n",
    "f = grp.count().word\r\n",
    "f.plot()\r\n",
    "plt.annotate(\r\n",
    "    f\"Busiest period ({f.idxmax()})\\n{f.max()} new words\", xy=(f.idxmax(), f.max())\r\n",
    ")\r\n",
    "busiest = grp.get_group(f.idxmax())\r\n",
    "busiest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\", \".join(busiest.word.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_per_period = []\r\n",
    "index = []\r\n",
    "grp = firsts_df.groupby(pd.Grouper(freq=\"m\"))\r\n",
    "# for name, fd in  grp:\r\n",
    "#     print(df.sender_name.value_counts())\r\n",
    "for period, df in grp:\r\n",
    "    index.append(period)\r\n",
    "    vc = df.by.value_counts()\r\n",
    "    people_per_period.append(vc[:int(len(vc)*0.2)].to_dict())\r\n",
    "\r\n",
    "new_pp_df = pd.DataFrame(people_per_period, index=index)\r\n",
    "new_pp_df.drop([\"Ben Doherty\"], inplace=True, axis=\"columns\", errors=\"ignore\")\r\n",
    "new_pp_df.head(3)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pp_df.plot.area(stacked=True)\r\n",
    "plt.xlim([\"2013-01-01\", \"2021-07-01\"])\r\n",
    "plt.legend(ncol=5)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_per_period = []\r\n",
    "index = []\r\n",
    "grp = firsts_df.groupby(pd.Grouper(freq=\"2m\"))\r\n",
    "for period, df in grp:\r\n",
    "    index.append(period)\r\n",
    "    vc = df.by.value_counts()\r\n",
    "    people_per_period.append(vc[:5].to_dict())\r\n",
    "\r\n",
    "new_pp_df = pd.DataFrame(people_per_period, index=index)\r\n",
    "new_pp_df.drop([\"Ben Doherty\"], inplace=True, axis=\"columns\", errors=\"ignore\")\r\n",
    "ax = new_pp_df.plot.bar(stacked=True)\r\n",
    "# plt.xlim([\"2013-01-01\", \"2021-07-01\"])\r\n",
    "\r\n",
    "plt.legend(ncol=5)\r\n",
    "for container in ax.containers:\r\n",
    "    # customize the label to account for cases when there might not be a bar section\r\n",
    "    labels = [f\"{h:.0f}\" if (h := v.get_height()) > 25 else \"\" for v in container]\r\n",
    "\r\n",
    "    # set the bar label\r\n",
    "    ax.bar_label(container, labels=labels, label_type=\"center\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_ranked = 20\r\n",
    "freq = \"4m\"\r\n",
    "\r\n",
    "grp = firsts_df.groupby(pd.Grouper(freq=freq))\r\n",
    "d = []\r\n",
    "for period, df in grp:\r\n",
    "    index.append(period)\r\n",
    "    vc = df.value_counts(subset=[\"by\", \"intials\"])\r\n",
    "    # print(vc, type(vc))\r\n",
    "    # vci = vc.index\r\n",
    "    # intials = df.intials[0] if df.shape[0]>0 else \"?\"\r\n",
    "    for i,((name, initials), value) in enumerate(vc.iteritems()):\r\n",
    "        # print(i, name, initials, value)\r\n",
    "        d.append(\r\n",
    "            {\r\n",
    "                \"period\": period,\r\n",
    "                \"name\": name,\r\n",
    "                \"rank\": i + 1,\r\n",
    "                \"intials\": initials,\r\n",
    "            }\r\n",
    "        )\r\n",
    "\r\n",
    "\r\n",
    "df = pd.DataFrame(d)\r\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sources = df[df[\"period\"] == df[\"period\"].max()].nsmallest(n_top_ranked, \"rank\")\r\n",
    "\r\n",
    "fig, ax = plt.subplots(\r\n",
    "    # figsize=(8, 5),\r\n",
    "    subplot_kw=dict(ylim=(0.5, 0.5 + n_top_ranked)),\r\n",
    ")\r\n",
    "\r\n",
    "ax.xaxis.set_major_locator(MultipleLocator(365))\r\n",
    "ax.yaxis.set_major_locator(MultipleLocator(1))\r\n",
    "\r\n",
    "yax2 = ax.secondary_yaxis(\"right\")\r\n",
    "yax2.yaxis.set_major_locator(FixedLocator(top_sources[\"rank\"].to_list()))\r\n",
    "yax2.yaxis.set_major_formatter(FixedFormatter(top_sources[\"name\"].to_list()))\r\n",
    "\r\n",
    "for name, name_df in df.groupby(\"name\"):\r\n",
    "    if not name_df.empty:\r\n",
    "        marker_initials = f\"${name_df.intials.iloc[0]}$\"\r\n",
    "        if name in top_sources.name.to_list():\r\n",
    "            ls = random.sample([\"-\", \"--\", \"-.\", \":\"], 1)[0]\r\n",
    "            markersize = 15\r\n",
    "            lw = 5\r\n",
    "        else:\r\n",
    "            ls = \"-\"\r\n",
    "            markersize = 8\r\n",
    "            lw = 1\r\n",
    "        ax.plot(\r\n",
    "            \"period\",\r\n",
    "            \"rank\",\r\n",
    "            marker=marker_initials,\r\n",
    "            markersize=markersize,\r\n",
    "            data=name_df,\r\n",
    "            mfc=\"w\",\r\n",
    "            lw=lw,\r\n",
    "            ls=ls,\r\n",
    "            solid_capstyle=\"round\",\r\n",
    "        )\r\n",
    "\r\n",
    "\r\n",
    "ax.invert_yaxis()\r\n",
    "ax.set(\r\n",
    "    xlabel=\"Period\",\r\n",
    "    ylabel=\"Rank\",\r\n",
    "    title=\"Ranking of number of new words introduced into the corpus\",\r\n",
    ")\r\n",
    "ax.grid(axis=\"x\")\r\n",
    "plt.tight_layout()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in firsts_df.groupby(\"by\"):\r\n",
    "    if name != MY_NAME and df.shape[0] > 300:\r\n",
    "        df.groupby(pd.Grouper(freq=\"q\")).by.count().plot(label=name)\r\n",
    "plt.legend()\r\n",
    "plt.title(\r\n",
    "    \"When do people introduce new words?\\n\"\r\n",
    "    \"(filtered by over 300 new words total, \"\r\n",
    "    \"aggregated over quarterly periods)\"\r\n",
    ")\r\n",
    "plt.xlim([\"2013-01-01\", \"2021-07-01\"])\r\n",
    "plt.ylim([0, 400])\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if there are a lot of words like \"heyyyyyy\"? Can we take them out?\r\n",
    "\r\n",
    "In this sample set, it leaves 1659 of 2946"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune this number, more repetitive numbers have bigger numbers, so if you want\r\n",
    "# to leave them in, increase it. To see the silly words, flip the comparison\r\n",
    "# from < to >, and to see the scores, swap the x for the line above it.\r\n",
    "thresh = 1.6\r\n",
    "# person = \"Meike Wijers\"\r\n",
    "# person = \"Ivana Kuzmanovska\"\r\n",
    "person = \"Byron Sullivan\"\r\n",
    "person_words = firsts_df[firsts_df.by == person].word.to_list()\r\n",
    "# (len(x) / len(set(x)), x)\r\n",
    "real_words = [x for x in person_words if len(x) < (len(set(x)) * thresh)]\r\n",
    "silly_words = [x for x in person_words if len(x) > (len(set(x)) * thresh)]\r\n",
    "print(\r\n",
    "    '\\n\"real\" words:',\r\n",
    "    \", \".join(real_words),\r\n",
    "    '\\n\\n\"silly\" words:',\r\n",
    "    \", \".join(silly_words),\r\n",
    ")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"words_dictionary.json\") as d:\r\n",
    "    the_dictionary = json.load(d)\r\n",
    "firsts_df[\"in_the_dictionary\"] = firsts_df.word.apply(\r\n",
    "    lambda x: \"yes\" if the_dictionary.get(x) else \"no\"\r\n",
    ")\r\n",
    "firsts_df.sample(3)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_real = {}\r\n",
    "real_words_dicts = []\r\n",
    "for name, df in firsts_df.groupby(\"by\"):\r\n",
    "    num_messages = df.shape[0]\r\n",
    "    vc = df.in_the_dictionary.value_counts()\r\n",
    "    num_in_dict = vc[\"yes\"]\r\n",
    "    pc_real[name] = (num_in_dict / num_messages) * 100\r\n",
    "    amc = all_convo_df[all_convo_df.sender_name == name].shape[0]\r\n",
    "    real_words_dicts.append(\r\n",
    "        {\r\n",
    "            \"name\": name,\r\n",
    "            \"total\": num_messages,\r\n",
    "            \"real_count\": num_in_dict,\r\n",
    "            \"unreal_count\": vc[\"no\"],\r\n",
    "            \"pc\": (num_in_dict / num_messages) * 100,\r\n",
    "            \"all_message_count\": amc,\r\n",
    "        }\r\n",
    "    )\r\n",
    "real_words_df = pd.DataFrame(real_words_dicts)\r\n",
    "real = pd.Series(pc_real).sort_values(ascending=False)\r\n",
    "real.plot.barh()\r\n",
    "# real.head(50)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_words_df[real_words_df.total>200].sort_values(\"pc\", ascending=False).tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "fig = plt.figure()\r\n",
    "ax = plt.axes(projection='3d')\r\n",
    "\r\n",
    "ax.scatter3D(real_words_df.real_count, real_words_df.unreal_count, real_words_df.all_message_count)\r\n",
    "plt.xlim([0, 1100])\r\n",
    "plt.ylim([0, 2500])\r\n",
    "ax.set_zlim(0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "# real_words_df.plot.scatter(\"real_count\", \"unreal_count\")\r\n",
    "from mpl_toolkits import mplot3d\r\n",
    "\r\n",
    "fig = plt.figure()\r\n",
    "ax = plt.axes(projection='3d')\r\n",
    "\r\n",
    "for i,row in real_words_df.iterrows():\r\n",
    "    ax.scatter(xs=row.real_count, ys=row.unreal_count, zs=row.all_message_count , label=row[\"name\"])\r\n",
    "    if row.real_count>100 or row.unreal_count>250:\r\n",
    "        ax.annotate(row[\"name\"], (row.real_count, row.unreal_count))\r\n",
    "plt.xlim([0, 1100])\r\n",
    "plt.ylim([0, 2500])\r\n",
    "ax.set_zlim(0, 10000)\r\n",
    "plt.xlabel(\"count of words found in the dictionary\")\r\n",
    "plt.ylabel(\"count of words not found in the dictionary\")\r\n",
    "ax.set_zlabel(\"count of all messages sent by this person\")\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\", \".join(\r\n",
    "    firsts_df[\r\n",
    "        (firsts_df.by == \"Charles Ogilvie\") & (firsts_df.in_the_dictionary == \"no\")\r\n",
    "    ].word.to_list()\r\n",
    ")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\r\n",
    "for period, df in firsts_df.groupby(pd.Grouper(freq=\"m\")):\r\n",
    "    d[period] = [x for x in df.word.to_list() if x.isnumeric() is False]\r\n",
    "words_in_period = pd.DataFrame.from_dict(d, orient=\"index\").T\r\n",
    "# TODO: replace none with \"\" so this prints in a nice looking way\r\n",
    "words_and_months = words_in_period.applymap(lambda x: \"\" if x is None else x)\r\n",
    "words_and_months.to_csv(\"words_and_months.csv\")\r\n",
    "words_and_months\r\n",
    "# The idea here was to make a printed bar chart where the words were the bars, \r\n",
    "# but at A0, each line is about 0.3mm high, so the word is about half that."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bc9a1eeff4ba10b0800c01e5b0b872b265b92561193d5706117af22821f4cc2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('dp-env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
