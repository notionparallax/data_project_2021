{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the conversations\r\n",
    "\r\n",
    "This is a bit trickier as you need to do something with all the conversations you're loading up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "Load up a tonne of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\r\n",
    "import json\r\n",
    "import os\r\n",
    "import pickle\r\n",
    "import random\r\n",
    "import re\r\n",
    "import textwrap\r\n",
    "from pathlib import Path\r\n",
    "from collections import OrderedDict\r\n",
    "\r\n",
    "import matplotlib as mpl\r\n",
    "import matplotlib.dates as mdates\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from matplotlib.font_manager import FontProperties\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import seaborn as sns\r\n",
    "from scipy.optimize import curve_fit\r\n",
    "from scipy.spatial import ConvexHull\r\n",
    "\r\n",
    "import message_helpers as mh\r\n",
    "from hangouts_loader import load_hangouts\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\r\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Segoe UI Emoji\"]\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_name = \"all_convo.pickle\"\r\n",
    "pickle_path = Path(pickle_name)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your name here. This is so that you can take yourself out of some of the graphs. Because these are conversations, naievely, they go A B A B and so on, so you'll be roughly 50% of the messages, which makes other trends hard to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_NAME = \"Ben Doherty\"\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_obj(obj):\r\n",
    "    \"\"\"Unfuck facebook's message storage encoding.\r\n",
    "\r\n",
    "    Facebook stores their messages in some kind of insane mix of latin 1 and utf-8\r\n",
    "    This means that emoji and iOS punctuation are broken unless decoded with this.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        obj (dict): a part of a facebook message\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        dict: tht object, but not munted\r\n",
    "    \"\"\"\r\n",
    "    for key in obj:\r\n",
    "        if isinstance(obj[key], str):\r\n",
    "            obj[key] = obj[key].encode(\"latin_1\").decode(\"utf-8\")\r\n",
    "        elif isinstance(obj[key], list):\r\n",
    "            obj[key] = list(\r\n",
    "                map(\r\n",
    "                    lambda x: x\r\n",
    "                    if type(x) != str\r\n",
    "                    else x.encode(\"latin_1\").decode(\"utf-8\"),\r\n",
    "                    obj[key],\r\n",
    "                )\r\n",
    "            )\r\n",
    "        pass\r\n",
    "    return obj\r\n",
    "\r\n",
    "\r\n",
    "def sumarise_convo(name, data, verbose=False):\r\n",
    "    words = {}\r\n",
    "    words[name] = data.content.str.cat(sep=\" \")\r\n",
    "    wordcount = len(words[name].split(\" \"))\r\n",
    "\r\n",
    "    unique_words = set(words[name].split(\" \"))\r\n",
    "\r\n",
    "    pool = \" \".join([str(x) for x in data.content.to_list()]).lower()\r\n",
    "    clean = re.sub(mh.PUNCTUATION_REGEX, \" \", pool, flags=re.VERBOSE)\r\n",
    "    # and replace it with a single space\r\n",
    "    stopped = list(set([w for w in clean.split() if w not in mh.STOP_WORDS]))\r\n",
    "\r\n",
    "    if verbose:\r\n",
    "        print(\r\n",
    "            f\"{name} wrote {wordcount} words ({len(words[name])} characters)\"\r\n",
    "            f\" and used {len(stopped)} different words.\"\r\n",
    "        )\r\n",
    "    return {\r\n",
    "        \"participant\": name,\r\n",
    "        \"wordcount\": wordcount,\r\n",
    "        \"unique_words\": len(unique_words),\r\n",
    "        \"cleaned_unique\": len(stopped),\r\n",
    "    }\r\n",
    "\r\n",
    "\r\n",
    "def get_message_length(message):\r\n",
    "    if type(message) is str:\r\n",
    "        return len(message)\r\n",
    "    else:\r\n",
    "        return len(str(message))\r\n",
    "\r\n",
    "\r\n",
    "def replace_typographic_apostrophy(message):\r\n",
    "    if type(message) is str:\r\n",
    "        return message.replace(\"’\", \"'\")\r\n",
    "    else:\r\n",
    "        return message\r\n",
    "\r\n",
    "def load_whole_inbox(rootdir, platform=\"Facebook\"):\r\n",
    "    conversations = []\r\n",
    "    for d in os.listdir(rootdir):\r\n",
    "        conversations.append(d)\r\n",
    "    print(f\"There are {len(conversations)} conversations to look at from {platform}.\")\r\n",
    "    # conversations\r\n",
    "\r\n",
    "    convo_df_list = []\r\n",
    "\r\n",
    "    \r\n",
    "    for convo in os.listdir(rootdir):\r\n",
    "        for f in os.listdir(os.path.join(rootdir, convo)):\r\n",
    "            try:\r\n",
    "                message_list = []\r\n",
    "                path = os.path.join(os.path.join(rootdir, convo, f))\r\n",
    "                if Path(path).is_file():\r\n",
    "                    with open(path, \"r\") as fb_data:\r\n",
    "                        messages = json.load(fb_data, object_hook=parse_obj)\r\n",
    "                        message_list.extend(messages[\"messages\"])\r\n",
    "\r\n",
    "                if len(message_list) != 0:\r\n",
    "                    df = pd.DataFrame(message_list)\r\n",
    "                    df[\"source_convo\"] = convo\r\n",
    "                    df[\"datetime\"] = df.timestamp_ms.apply(\r\n",
    "                        lambda x: datetime.datetime.fromtimestamp(x / 1000.0)\r\n",
    "                    )\r\n",
    "\r\n",
    "                    if \"content\" in df.columns:\r\n",
    "                        df[\"message_length\"] = df.content.apply(get_message_length)\r\n",
    "                        df.content = df.content.apply(\r\n",
    "                            replace_typographic_apostrophy\r\n",
    "                        )\r\n",
    "                    else:\r\n",
    "                        df[\"message_length\"] = 0\r\n",
    "                        df[\"content\"] = np.nan\r\n",
    "\r\n",
    "                    df[\"platform\"] = platform\r\n",
    "\r\n",
    "                    convo_df_list.append(df)\r\n",
    "\r\n",
    "            except Exception as e:\r\n",
    "                print(\"exception\", convo, e)\r\n",
    "    print(f\"finished with {platform}\")\r\n",
    "    return convo_df_list\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_rootdir = \"fb_data\\messages\\inbox\"\r\n",
    "ig_rootdir = \"ig_data\\inbox\"\r\n",
    "\r\n",
    "\r\n",
    "print(\"Loading from source files\")\r\n",
    "fb_convo_df_list = load_whole_inbox(fb_rootdir, platform=\"Facebook\")\r\n",
    "ig_convo_df_list = load_whole_inbox(ig_rootdir, platform=\"Instagram\")\r\n",
    "ho_convo_df_list = [load_hangouts()]\r\n",
    "\r\n",
    "convo_df_list = fb_convo_df_list + ig_convo_df_list + ho_convo_df_list\r\n",
    "# print(len(convo_df_list))\r\n",
    "all_convo_df = pd.concat(convo_df_list)\r\n",
    "print(f\"done: all_convo_df has {all_convo_df.shape[0]} rows\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\r\n",
    "    f\"Overall, there are {len(all_convo_df)}, messages in this dataset. \"\r\n",
    "    f\"These come from about {len(all_convo_df.sender_name.unique())} people, \"\r\n",
    "    f\"covering a period of {str(all_convo_df.datetime.max()-all_convo_df.datetime.min()).split(' days')[0]} days \"\r\n",
    "    f\"between {all_convo_df.datetime.min():%B, %Y} and {all_convo_df.datetime.max():%B, %Y}. \"\r\n",
    "    f\"Over {len(all_convo_df.platform.unique())} platforms:\"\r\n",
    ")\r\n",
    "all_convo_df.platform.value_counts()\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge names\r\n",
    "\r\n",
    "Some people have different names across different platforms, firstly let's get a list of unique names, and then define a dictionary that will say what to merge into what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\r\n",
    "platform_names = {}\r\n",
    "for platform, df in all_convo_df.groupby(\"platform\"):\r\n",
    "    vc = df.sender_name.value_counts()\r\n",
    "    these_names = vc[vc > 100].index.to_list()\r\n",
    "    names += these_names\r\n",
    "    print(\"\\n\",platform, \"\\n\",these_names)\r\n",
    "    platform_names[platform] = set(these_names)\r\n",
    "# print(platform_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_s = set(platform_names[\"Facebook\"])\r\n",
    "ig_s = set(platform_names[\"Instagram\"])\r\n",
    "ho_s = set(platform_names[\"Hangouts\"])\r\n",
    "# print(fb_s.symmetric_difference(ig_s))\r\n",
    "# print(fb_s.intersection(ig_s))\r\n",
    "print(\"\\nshows in all platforms\", fb_s & ig_s & ho_s)\r\n",
    "print(\"\\nshows in fb and ig\", fb_s & ig_s)\r\n",
    "print(\"\\nfb only\", fb_s - ig_s)\r\n",
    "print(\"\\nig only\", ig_s - fb_s)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\r\n",
    "Use the lists above to work out who shows up in more than one list, but under different names, and then use the thesaurus below to map their names to a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_names(input_name):\r\n",
    "    annisa = \"Annisa Rivera Rizal\"\r\n",
    "    byron = \"Byron Sullivan\"\r\n",
    "    charlie = \"Charles Ogilvie\"\r\n",
    "    clarrie = \"Clarrie Morabito\"\r\n",
    "    ivana = \"Ivana Kuzmanovska\"\r\n",
    "    jess = \"Jess Howard\"\r\n",
    "    jodie = \"Jodie Hinton\"\r\n",
    "    julz = \"Jülz Milthorpe\"\r\n",
    "    karin = \"Karin Ke\"\r\n",
    "    tones = \"Antonia Sheil\"\r\n",
    "    thesaurus = {\r\n",
    "        \"Byron Sullivan\": byron,\r\n",
    "        \"Byron\": byron,\r\n",
    "        \"Thearlaich Ogilive\": charlie,\r\n",
    "        \"Charles OGILVIE\": charlie,\r\n",
    "        \"Charles Ogilvie\": charlie,\r\n",
    "        \"Karin Frost\": karin,\r\n",
    "        \"karin ke\": karin,\r\n",
    "        \"Ivana Kuzmanovska\": ivana,\r\n",
    "        \"ivana kuzmanovska\": ivana,\r\n",
    "        \"Jülz\": julz,\r\n",
    "        \"Jülz Milthorpe\": julz,\r\n",
    "        \"jesshoward\": jess,\r\n",
    "        \"Jess Howard\": jess,\r\n",
    "        \"Jodie\": jodie,\r\n",
    "        \"Tones\": tones,\r\n",
    "        \"annisarivera\": annisa,\r\n",
    "        \"Clarrie\": clarrie,\r\n",
    "        \"tanyaruby\": \"Tanya P\",\r\n",
    "        \"iflaneuse\": \"Nicole Gardiner\",\r\n",
    "        \"Frank\": \"Francis Lockie\",\r\n",
    "    }\r\n",
    "    new_name = thesaurus.get(input_name, input_name)\r\n",
    "    # if new_name != input_name:\r\n",
    "    #     print(f\"renamed {input_name} to {new_name}\")\r\n",
    "    return new_name\r\n",
    "\r\n",
    "\r\n",
    "all_convo_df[\"input_names\"] = all_convo_df.sender_name\r\n",
    "all_convo_df.sender_name = all_convo_df.sender_name.apply(fold_names)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initials\r\n",
    "\r\n",
    "To label a lot of graphs we'll need a compact way to represent people, so I (Ben Doherty) can be shown as BD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_initials = all_convo_df.sender_name.value_counts().to_dict()\r\n",
    "initials_pool = []\r\n",
    "for name in all_initials.keys():\r\n",
    "    split = [x for x in re.split(\"[ \\.-]\", name) if x != \"\"]\r\n",
    "    try:\r\n",
    "        initials = \"\".join([n[0] for n in split]).upper()\r\n",
    "        if initials in initials_pool:\r\n",
    "            # print(\"uh oh, double up on\", initials, name, initials_pool)\r\n",
    "            initials = \"\".join([n[0] for n in split]).upper() + name.split()[-1][1]\r\n",
    "            # print(\"replaced with\", initials)\r\n",
    "            if initials in initials_pool:\r\n",
    "                initials = (\r\n",
    "                    split[0].upper()\r\n",
    "                    + split[1].lower()\r\n",
    "                    + split[-1].upper()\r\n",
    "                    + split[-2].lower()\r\n",
    "                )\r\n",
    "                if initials in initials_pool:\r\n",
    "                    print(\"fuck, complicated\", name)\r\n",
    "                    initials = name+\"x\"\r\n",
    "    except Exception as e:\r\n",
    "        # print(name, e, split)\r\n",
    "        initials = name+\"x\"\r\n",
    "    all_initials[name] = initials\r\n",
    "    initials_pool.append(initials)\r\n",
    "\r\n",
    "all_convo_df[\"initials\"] = all_convo_df.sender_name.apply(lambda x: all_initials[x])\r\n",
    "all_convo_df[[\"sender_name\", \"content\", \"initials\"]].sample(5)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender\r\n",
    "\r\n",
    "I've had to make a new table, and just make up what people are assigning, but I think it's basically right. This will show if I'm biased in one way or the other in my messaging.\r\n",
    "\r\n",
    "I've put myself in with a gender of `me` so that I can tell where the split really lies, as I'm &thickapprox; 50% of the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_df = pd.read_csv(\"gender.csv\")\r\n",
    "all_convo_df = pd.merge(all_convo_df, gender_df, on=\"sender_name\")\r\n",
    "all_convo_df.sample(4)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_stop(content, as_list=False):\r\n",
    "    try:\r\n",
    "        clean = re.sub(\r\n",
    "            mh.PUNCTUATION_REGEX,\r\n",
    "            \" \",\r\n",
    "            content,\r\n",
    "            flags=re.VERBOSE,  # and replace it with a single space\r\n",
    "        )\r\n",
    "        stopped = [w.lower() for w in clean.split() if w.lower() not in mh.STOP_WORDS]\r\n",
    "        # print(content, \"=>\", stopped)\r\n",
    "        if as_list:\r\n",
    "            return stopped\r\n",
    "        else:\r\n",
    "            return \" \".join(stopped)\r\n",
    "    except Exception as e:\r\n",
    "        # print(content, e)\r\n",
    "        return content\r\n",
    "\r\n",
    "\r\n",
    "all_convo_df[\"clean_content\"] = all_convo_df.content.apply(clean_and_stop)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"saving new pickle: {pickle_path}\")\r\n",
    "pd.to_pickle(all_convo_df, pickle_path)\r\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bc9a1eeff4ba10b0800c01e5b0b872b265b92561193d5706117af22821f4cc2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('dp-env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
